#!/usr/bin/env python3
"""
Script de prueba para validar el sistema de confianza detallada mejorado.
Tarea 4.3: Add prediction confidence scores to API response
"""

import sys
import os
import json
import asyncio
from datetime import datetime
from typing import Dict, Any

# Agregar el directorio padre al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.adaptive_ui_service import AdaptiveUIService
from app.models.adaptive_ui import UserContext


async def test_confidence_system():
    """Prueba integral del sistema de confianza detallada."""
    
    print("üß™ Probando sistema de confianza detallada (Tarea 4.3)")
    print("=" * 60)
    
    # Inicializar servicio
    service = AdaptiveUIService()
    
    # Contextos de prueba diversos
    test_contexts = [
        {
            "name": "Desktop High-End",
            "context": UserContext(
                hora_local=datetime.now(),
                prefers_color_scheme="light",
                viewport_width=1920,
                viewport_height=1080,
                touch_enabled=False,
                device_pixel_ratio=1.0,
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                session_id="test-desktop-001",
                page_path="/test"
            )
        },
        {
            "name": "Mobile Dark Mode",
            "context": UserContext(
                hora_local=datetime.now(),
                prefers_color_scheme="dark",
                viewport_width=375,
                viewport_height=812,
                touch_enabled=True,
                device_pixel_ratio=3.0,
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)",
                session_id="test-mobile-001",
                page_path="/test"
            )
        },
        {
            "name": "Tablet Medium",
            "context": UserContext(
                hora_local=datetime.now(),
                prefers_color_scheme="no-preference",
                viewport_width=768,
                viewport_height=1024,
                touch_enabled=True,
                device_pixel_ratio=2.0,
                user_agent="Mozilla/5.0 (iPad; CPU OS 14_0 like Mac OS X)",
                session_id="test-tablet-001",
                page_path="/test"
            )
        }
    ]
    
    test_results = []
    
    # Ejecutar pruebas
    for i, test_case in enumerate(test_contexts, 1):
        print(f"\nüì± Prueba {i}/3: {test_case['name']}")
        print("-" * 40)
        
        try:
            # Generar predicci√≥n con confianza detallada
            response = await service.generate_adaptive_design(
                user_context=test_case["context"],
                user_id="test-user-confidence",
                is_authenticated=True
            )
            
            # Extraer informaci√≥n de confianza
            confidence = response.prediction_confidence
            
            # Validar estructura de confianza
            structure_valid = validate_confidence_structure(confidence)
            
            test_result = {
                "test_name": test_case["name"],
                "success": True,
                "structure_valid": structure_valid,
                "confidence_data": confidence,
                "design_tokens": {
                    "css_classes": response.design_tokens.css_classes,
                    "css_variables": list(response.design_tokens.css_variables.keys())
                },
                "processing_time": response.processing_time_ms
            }
            
            # Mostrar resultados
            print_confidence_summary(confidence, test_case["name"])
            
        except Exception as e:
            print(f"‚ùå Error en prueba {test_case['name']}: {e}")
            test_result = {
                "test_name": test_case["name"],
                "success": False,
                "error": str(e)
            }
        
        test_results.append(test_result)
    
    # Resumen final
    print("\n" + "=" * 60)
    print("üìä RESUMEN DE PRUEBAS DE CONFIANZA")
    print("=" * 60)
    
    successful_tests = sum(1 for result in test_results if result.get("success", False))
    total_tests = len(test_results)
    
    print(f"‚úÖ Pruebas exitosas: {successful_tests}/{total_tests}")
    
    if successful_tests > 0:
        # An√°lisis de diversidad de confianza
        analyze_confidence_diversity(test_results)
        
        # Validaci√≥n de m√©tricas
        validate_confidence_metrics(test_results)
    
    return successful_tests == total_tests


def validate_confidence_structure(confidence: Dict[str, Any]) -> bool:
    """Valida que la estructura de confianza sea correcta."""
    
    required_keys = ["classification", "regression", "overall", "detailed"]
    
    # Verificar claves principales
    if not all(key in confidence for key in required_keys):
        print("‚ùå Estructura de confianza incompleta")
        return False
    
    # Verificar estructura de clasificaci√≥n
    classification = confidence["classification"]
    if isinstance(classification, dict):
        required_class_keys = ["score", "quality", "metrics", "prediction_certainty"]
        if not all(key in classification for key in required_class_keys):
            print("‚ùå Estructura de confianza de clasificaci√≥n incompleta")
            return False
    
    # Verificar estructura de regresi√≥n
    regression = confidence["regression"]
    if isinstance(regression, dict):
        required_reg_keys = ["score", "quality", "metrics"]
        if not all(key in regression for key in required_reg_keys):
            print("‚ùå Estructura de confianza de regresi√≥n incompleta")
            return False
    
    # Verificar estructura detallada
    detailed = confidence["detailed"]
    required_detailed_keys = ["classifier_quality", "regressor_quality", "combined_quality"]
    if not all(key in detailed for key in required_detailed_keys):
        print("‚ùå Estructura de confianza detallada incompleta")
        return False
    
    print("‚úÖ Estructura de confianza v√°lida")
    return True


def print_confidence_summary(confidence: Dict[str, Any], test_name: str):
    """Imprime un resumen legible de la confianza."""
    
    print(f"üìä Confianza para {test_name}:")
    
    # Overall score
    overall = confidence.get("overall", 0)
    print(f"   üéØ Score general: {overall:.1f}%")
    
    # Clasificaci√≥n
    classification = confidence.get("classification", {})
    if isinstance(classification, dict):
        class_score = classification.get("score", 0)
        class_quality = classification.get("quality", "unknown")
        class_certainty = classification.get("prediction_certainty", "unknown")
        print(f"   üè∑Ô∏è  Clasificaci√≥n: {class_score:.1f}% ({class_quality}, {class_certainty})")
    else:
        print(f"   üè∑Ô∏è  Clasificaci√≥n: {classification:.1f}% (legacy)")
    
    # Regresi√≥n
    regression = confidence.get("regression", {})
    if isinstance(regression, dict):
        reg_score = regression.get("score", 0)
        reg_quality = regression.get("quality", "unknown")
        print(f"   üìà Regresi√≥n: {reg_score:.1f}% ({reg_quality})")
    else:
        print(f"   üìà Regresi√≥n: {regression:.1f}% (legacy)")
    
    # Detalles adicionales
    detailed = confidence.get("detailed", {})
    if detailed:
        combined_quality = detailed.get("combined_quality", "unknown")
        print(f"   üîó Calidad combinada: {combined_quality}")
        
        reliability = detailed.get("reliability_summary", {})
        if reliability:
            trustworthiness = reliability.get("overall_trustworthiness", "unknown")
            print(f"   üõ°Ô∏è  Confiabilidad: {trustworthiness}")


def analyze_confidence_diversity(test_results):
    """Analiza la diversidad en los scores de confianza."""
    
    print("\nüîç AN√ÅLISIS DE DIVERSIDAD DE CONFIANZA")
    print("-" * 40)
    
    scores = []
    qualities = []
    
    for result in test_results:
        if result.get("success") and "confidence_data" in result:
            confidence = result["confidence_data"]
            overall = confidence.get("overall", 0)
            scores.append(overall)
            
            detailed = confidence.get("detailed", {})
            quality = detailed.get("combined_quality", "unknown")
            qualities.append(quality)
    
    if scores:
        avg_score = sum(scores) / len(scores)
        min_score = min(scores)
        max_score = max(scores)
        score_range = max_score - min_score
        
        print(f"üìä Scores de confianza:")
        print(f"   ‚Ä¢ Promedio: {avg_score:.1f}%")
        print(f"   ‚Ä¢ Rango: {min_score:.1f}% - {max_score:.1f}%")
        print(f"   ‚Ä¢ Variaci√≥n: {score_range:.1f}%")
        
        print(f"üéØ Calidades observadas: {set(qualities)}")
        
        # Verificar diversidad
        if score_range > 10:
            print("‚úÖ Diversidad de confianza adecuada")
        else:
            print("‚ö†Ô∏è Baja diversidad en scores de confianza")


def validate_confidence_metrics(test_results):
    """Valida que las m√©tricas de confianza sean sensatas."""
    
    print("\nüî¨ VALIDACI√ìN DE M√âTRICAS")
    print("-" * 40)
    
    valid_metrics = 0
    total_metrics = 0
    
    for result in test_results:
        if result.get("success") and "confidence_data" in result:
            confidence = result["confidence_data"]
            
            # Validar score overall
            overall = confidence.get("overall", 0)
            if 0 <= overall <= 100:
                valid_metrics += 1
            total_metrics += 1
            
            # Validar scores individuales
            classification = confidence.get("classification", {})
            if isinstance(classification, dict):
                class_score = classification.get("score", 0)
                if 0 <= class_score <= 100:
                    valid_metrics += 1
                total_metrics += 1
            
            regression = confidence.get("regression", {})
            if isinstance(regression, dict):
                reg_score = regression.get("score", 0)
                if 0 <= reg_score <= 100:
                    valid_metrics += 1
                total_metrics += 1
    
    print(f"üìê M√©tricas v√°lidas: {valid_metrics}/{total_metrics}")
    
    if valid_metrics == total_metrics:
        print("‚úÖ Todas las m√©tricas est√°n en rangos v√°lidos")
    else:
        print("‚ö†Ô∏è Algunas m√©tricas fuera de rango")


async def main():
    """Funci√≥n principal."""
    try:
        success = await test_confidence_system()
        
        if success:
            print("\nüéâ TODAS LAS PRUEBAS DE CONFIANZA EXITOSAS")
            print("‚úÖ Sistema de confianza detallada funcionando correctamente")
            exit_code = 0
        else:
            print("\n‚ùå ALGUNAS PRUEBAS FALLARON")
            exit_code = 1
            
    except Exception as e:
        print(f"\nüí• Error cr√≠tico en pruebas: {e}")
        exit_code = 1
    
    return exit_code


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    print(f"\nüèÅ Script finalizado con c√≥digo: {exit_code}")
    sys.exit(exit_code)